# Index
- [Constrained Level Generation](#clg)
- [Machine Configuration](#config)
- [Training Data](#data)
- [Reachability](#reachab)
    - [Collect data](#rdata)
    -Â [reachability.py & level_to_reachability.py](#ltr)
    - [Create and train the CNN](#cnn)
    - [Semantic loss](#rsl)
    - [Results](#rresults)
- [Onehot](#onehot)
    - [Semantic loss](#osl)
    - [Results](#oresults)
- [Pipes](#pipes)
    - [Semantic loss](#psl)
    - [Results](#presults)
- [Monsters](#monsters)
    - [Semantic loss](#msl)
    - [Results](#mresults)
- [Cannons](#cannons)
    - [Semantic loss](#csl)
    - [Results](#cresults)


This file is not up-to-date

<a name="clg"></a>  
## Constrained Level Generation
This project aims at improving the quality of levels generated by the paper [Evolving Mario Levels in the Latent Space of a Deep Convolutional Generative Adversarial Network](https://arxiv.org/abs/1805.00728). Using the power of **constrained** training, we try to create better levels by instructing the network to create maps with some desiderable properties, like generating levels that have a solution (reachability chapter) or create levels that do not contain malformed structures (e.g. chapter on pipes, cannons, ...). This is done by creating ad-hoc losses that are going to be added to the base loss of the generator in different ways and with different weights. To better express such constraints, we use semantic loss, that helps a lot in defining those constraints in propositional logic while keeping derivability. The semantic loss is proportional to the level of constraints unsatisfaction: penalizing invalid objects the generator is pushed to the creation of more valid levels.

To install all the frameworks that are required to use the semantic loss, please refer to the [semantic_loss](../semantic_loss/README.md) folder. 

<a name="config"></a>
## HW and SW Configuration
The whole set of packages used in this project are available in the `requirements.txt` file in this folder. The hardware configuration used is:

- CPU: Ryzen 7 1700X @ 3.4GHz
- GPU: GTX 1080Ti with 11GB of GDDR5 VRAM
- RAM: 32GB @ 3200MHz
- Ubuntu 16.04 LTS with CUDA 10, cuDNN 7.4.1 and JDK 1.8

The main deep learning framework used are `Tensorflow 1.13.1` and `PyTorch 1.1.0` with `TorchVision 0.3.0`. PyTorch has been used to run the models from the previous mentioned paper. `Java` has been used to run CMA-ES, as described, again, in the previous mentioned paper.

<a name="data"></a>
## Training data
The levels on which the network is trained are the ones from the [Video Game Level Corpus](https://github.com/TheVGLC/TheVGLC). At the moment, the focus is on the levels of `Super Mario Bros` only, but probably we will extend support at least to `Super Mario Bros 2`. The dataset folder contains both the original images of the levels and the *processed* ones in `txt` format, with a different character for each tile type.
The encoding used is showed in the following table:

| Character | Description |
| --- | --- |
| X | solid, ground |
| S | solid, breakable |
| - | passable, empty |
| ? | solid, question block, full question block |
| Q | solid, question block, empty question block |
| E | enemy, damaging, hazard, moving |
| < | solid, top-left pipe, pipe |
| > | solid, top-right pipe, pipe |
| \[ | solid, left pipe, pipe |
| \] | solid, right pipe, pipe |
| o | coin, collectable, passable |
| B | Cannon top, cannon, solid, hazard |
| b | Cannon bottom, cannon, solid |


# To be updated. From now, results and description are not up to date. Refer to the thesis for up to date details

<a name="reachab"></a>
## Reachability
The first constraint that we will study is **reachability**. Reachability tries to help in the generation of levels that have a solution (the difficulty of the level is not really important here). The **VGLC** contains both levels that have a lot of *passable* tiles (e.g. `mario-1-1` and `mario-4-1`) and levels that have more *solid* tiles. Moreover, there are levels with complex structures leading to a potentially higher risk of giving unsolvable (= not playable) solutions after the training (e.g. `mario-1-3` and `mario-3-3`).
We will call **perfect levels** those levels that exactly satisfy some constraints, e.g. reachability. Levels that given to the CANs as training set do already easily satisfy some constraints will not be taken into account (e.g. `mario-1-1`, given its simplicity, does satisfy reachability well without applying new techniques).
Given the difficulty in implementing a boolean constraint over the whole level encoding the reachability constriant, we apply an intermediate differentiable transformation using a CNN. This transformation maps a level to its reachability map on which then the semantic loss is applied.
Next chapters will gives details on:
- [Collect lot of levels distribuions and use them as samples to train the CNN](#rdata)
- [Approximate the reachability map of those distributions and use them as labels while training the CNN](#ltr)
- [Train the CNN](#cnn)
- [Create the semantic loss function](#rsl)
- [Check if there is a significant improvements](#rresults)


<a name="rdata"></a>
### Collect data
To create a network able to generalize a lot of different mario level types and different scenarios, level distributions has been extracted while training the `MarioGAN` on the whole **Super Mario Bros** level corpus. Then `40000` distributions has been randomly selected after having extracted `1200000` distributions from the network training, uniformily over the 5000 epochs. This has been done to improve diversity given that between two consecutive epochs there are not big changes in the generation of distribution samples.


<a name="ltr"></a>
### `reachability.py` & `level_to_reachability.py`
The first step to create a map between the distributions of level tiles and the corresponding reachability matrix has been to implement and fix some small bugs in the [VGLC platformer pathfinding algorithm](https://github.com/TheVGLC/TheVGLC/blob/master/PlatformerPathfinding/test_level.py). This algorithm takes a binary map and, given a file with all the allowed moves, it returns the binary map of the reachability. The problem is that we want an algorithm able to predict the distribution of the reachability map given the distribution of tiles of a certain level. To do so, our approach simply takes `k` (usually k=100) samples from the level distributions and finally it does the average of all the binary reachability maps to extract a good approximation of the reachability distribution.
In particular, `reachability.py` contains the adapted version of the reachability algorithm while the script `level_to_reachability_map.py` computes the average reachability maps of a given set of level distributions. Suppose you have a folder called `level_distributions`, containing a numpy `.npy` file for each level distribution: use the following script
```bash
python level_to_reachability_map.py -p <platformer_file> -i level_distributions -l level_distributions_binary -r reachability_maps
```
to create two new folders
- `level_distributions_binary`, containing the level distributions where probabilities of all **passable** and **solid** tiles are summed together;
- `reachability_map`, containing the reachability distributions of each level distribution given in input.

`level_distributions_binary` and `reachability_map` will become the input data and labels respectively to train the reachability CNN.


<a name="cnn"></a>
### Create and train the CNN
The best network architecture to approximate the reachability distribution is probably a convolutional neural network. Given the need to *expand* the information of reachability in the level dimension, we adopted an architecture composed by a sequence of convolutional layer with an increasing kernel size and an increasing number of filters. ReLU are used as activation functions and a final linear layer is applied to resize the output back to shape `[height, weight, 2]`.
You can find more details about the CNN architecture in [architectures.py](./architectures.py)
The current architecture is able to reach different performances based on the level on which it is tested. The best model we found is trained on the whole level corpus of the VGLC and with 70% of training data and 30% of test data is able to reach the following performances.

![Reachability CNN Results](pictures/results_reachability_cnn.png)

Unfortunately, the reachability map of some levels are not that simple to be approximated and can give some problems, but overall we didn't find a level whom reachability didn't improve of at least a small percentage.


<a name="rsl"></a>
### Semantic Loss
The semantic loss allow the writing of constraints as boolean formula in propositional logic. Given a derivable reachability map of a level distribution, we say that a level is reachable (e.g. has a practicable solution) if both the right- and left-bottom corners of the reachability map has a structure like:

| r |
| ---- |
|  r  |
|  n  |
| # |

or

| r |
| ---- |
|  n  |
|  n  |
| # |

with `#` as level borders, `r` as reachable blocks and  `n` as non-reachable blocks. 
The semantic loss is then an `OR` between the two possible configurations for each corner and an `AND` between the two constraints of each level.
This constraints encodes the concept: "a playable level should have a **reachable** platform on the left bottom part of the level from which mario can start to move and a **rechable** platform in the left bottom region of the level to allow the completion".


<a name="rresults"></a>
### Results
The following charts shows the reachability of samples using the VGLC algorithm. At each epoch, a batch of samples are generated by the generator: the charts show the percentage of objects that (after having computed their reachability map) satisfy the constraints define above.
Results are pretty good considering that most of the semantic losses with a moderate weight (< 0.005) does not really affect the normal training of the generator. Moreover, the results showed are taken against some of the harder levels (`mario-1-3` and `mario-3-3`). In the three experiments, the baselines are drawn in azure.

#### mario-1-3

In this case, the best option seems to use a SL weight of 0.005 that guarantees good performances and does not affect a lot the normal learning of structures of the generator.

| Gen. Loss Weight | SL Weight | SL from epoch | SL incremental | Color |
| ---- | ---- | ---- | ---- | ---- |
| 1.0 | 0.0001 | 3000 | yes | orange |
| 1.0 | 0.001 | 3000 | yes | blue |
| 1.0 | 0.005 | 2000 | yesÂ | rose |
| 1.0 | 0.01 | 3000 | yes| red |
| 1.0 | 0 | 0 | false | azure |

![Reachability CNN Results](pictures/mario-1-3-reachability.png)


#### mario-3-3

Again, the best option seems to be one of the experiment with a SL weight of 0.001 or 0.005.

| Gen. Loss Weight | SL Weight | SL from epoch | SL incremental | Color |
| ---- | ---- | ---- | ---- | ---- |
| 1.0 | 0.0001 | 3000 | yes | orange |
| 1.0 | 0.001 | 3000 | yes | blue |
| 1.0 | 0.005 | 2000 | yesÂ | rose |
| 1.0 | 0.01 | 3000 | yes| red |
| 1.0 | 0.0 | 0 | no | azure |

![Reachability CNN Results](pictures/mario-3-3-reachability.png)


#### mario-6-2

Finally, in this experiment too the best choice as a SL weight seems to be 0.005. `mario-6-2` is not a level full of jumps and possible death by fall as can be seen in the chart.

| Gen. Loss Weight | SL Weight | SL from epoch | SL incremental | Color |
| ---- | ---- | ---- | ---- | ---- |
| 1.0 | 0.0001 | 3000 | yes | orange |
| 1.0 | 0.001 | 3000 | yes | blue |
| 1.0 | 0.002 | 2500 | yes | green |
| 1.0 | 0.005 | 2000 | yesÂ | rose |
| 1.0 | 0.01 | 3000 | yes| red |
| 1.0 | 0.0 | 0 | no | azure |

![Reachability CNN Results](pictures/mario-6-2-reachability.png)



<a name="onehot"></a>
## Onehot
Onehot is a constraint that should help the generator to be less uncertain in chosing with tile type should be assigned to a pixel. Onehot requires that only a tile for each pixel has a high probability of being chosen instead of having different tiles with an comparable probability. The purpose of adding a semantic loss that helps in the generation of more robust distributions is that sampling will be more clean and free of artifacts that are not really appreciated. We hope that adding the onehot constraint, a distribution for a pixel with the following tile distribution 

| X | S | - | ? | Q | E | < | > | \[ | \] | o | B | b |
| - | - | - | - | - | - | - | - | - | - | - | - | - |
| 0.11 | 0.02 | 0.01 | 0.01 | 0.02 | 0.08 | 0.18 | 0.17 | 0.16 | 0.20 | 0.01 | 0.02 | 0.01 |

will become something like

| X | S | - | ? | Q | E | < | > | \[ | \] | o | B | b |
| - | - | - | - | - | - | - | - | - | - | - | - | - |
| 0.04 | 0.02 | 0.01 | 0.01 | 0.02 | 0.08 | 0.06 | 0.08 | 0.09 | 0.55 | 0.01 | 0.02 | 0.01 |

with a value that is really higher than the others.


<a name="osl"></a>
### Semantic loss
The semantic loss for this constraint is simply writable in DNF but not in CNF. Obviously, the DNF is the disjunction of conjuction like `X0 & ~X1 & ~X2 ...`, `~X0 & X1 & ~X2 ...`, `~X0 & ~X1 & X2 ...`, ... where at each step only one variable is allowed to be true. To create a CNF boolean formula with N variable, please use the script `level_generation/onehot_to_cnf.py` in the following way:
```bash
python onehot_to_cnf.py -i <number_of_variables> -o <output_file>
```
The CNF will be written with the `sympy` sintax.

<a name="oresults"></a>
### Results
!!! upload results !!!


<a name="pipes"></a>
## Pipes
Pipes are one of the most used structures in mario levels and often they are not recreated correctly by the level generator. Setting some constraints on their generation should improve their perfection and the overall quality of the level.
The general structure of pipes is:

| < | > |
| ---- | ---- |
| \[ | \] |
| \[ | \] |
| # | # |

Pipes can have a variable height. Characters `[` and `]` stand for the base, `<` and `>` stand for the top and `#` is generic ground.


<a name="psl"></a>
### Semantic loss
To avoid having too much constraints, we extracted small level pieces with size `[2, 2, 4]` with `stride=1`. The 4 channels are the ones for the 4 tile types that forms a pipe: `[`, `]`, `<` and `>`. Given the fixed width equal to 2 and the variable height of the pipes, we discovered that extracting pieces of size `[2, 2]` and moving of 1 single tile is sufficient to express all the important constraints. 

The constraints are:
- a `[` on the left implies a `]` on the right and viceversa, both on the first and the second row.
- a `<` on the left implies a `>` on the right and viceversa, both on the first and the second row.
- a `<` on the upper row implies an `[` below and a `>` on the upper row implies an `]` below.
- a `[` on the second row implies (`[` or `<`) above and a `]` on the second row implies (`]` or `>`) above.
The constraints that pipes should not fly can be learned automatically by the network because there are not flying pipes in the training set.



<a name="monsters"></a>
## Monsters
The constraint on mosters is very simple: create monsters that are not already flying at creation time. The constraint encodes the requirement that a monster sits on a solid block.

<a name="msl"></a>
### Semantic loss
Dividing each level in blocks of size `[2, 1]` (height, width), the constraint should be an impication from the probability in the top tile of being a monster to the probability in the bottom block of being solid (summed over all solid blocks).

<a name="mresults"></a>
### Results
Not tested yet.


<a name="cannons"></a>
## Cannons
Simple as the constraint on monsters, the constraint on cannons requires that a tile of type `b` (the cannon bottom) is always under a tile of type `B` (the cannon top). Given that `B` tiles can also live alone and stay directly on a solid block, an implication should be used.

<a name="csl"></a>
### Semantic loss
The level is divided in blocks on `[2, 1]` as in the monsters constraint and and implication like this is used: if the bottom tile is of type `b`, then the block above has to be of type `B`.

<a name="cresults"></a>
### Results
Not tested yet.




